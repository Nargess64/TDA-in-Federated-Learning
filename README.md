# TDA-in-Federated-Learning

Federated Learning (FL) has emerged as a transformative approach for training machine learning models
across decentralized data sources while keeping client data localized. Despite its advantages, FL systems
remain vulnerable to various attacks and anomalies, including model poisoning attacks, which compromise
the integrity of the global model. In this paper, we introduce a novel approach for detecting such attacks by
leveraging persistence diagrams derived from topological data analysis (TDA). Our method provides a comprehensive
solution for identifying anomalies in the training process by computing persistence diagrams in
high-dimensional spaces, effectively addressing the challenges of analyzing complex neural network architectures.
Through extensive experiments, we demonstrate that our approach achieves high accuracy in detecting
and mitigating attacks, even under non-IID and highly unbalanced data distribution scenarios. We evaluate our
method across various datasets and attack scenarios, and the results validate its robustness and effectiveness,
establishing it as a promising solution for enhancing the security of federated learning environments.
